\subsection{Signal model}
We model the deterministic part of the observed signal as a multicomponent harmonic signal, that is a sum of sine waves. Then:
\begin{equation}
\bz[n]=\sum_{j=1}^J\Omega_j\cos\left(2\pi f_j \frac{n}{\fs} + \varphi_j\right)\ ,
\label{eq:sum.sine}
\end{equation}
where $J$ denotes the number of components, $\Omega_j>0$ the amplitude of the $j$-th component, $f_j$ its frequency, and $\varphi_j\in[0,2\pi)$ its initial phase.

\begin{remark}
For the sake of simplicity, we make an additional assumption on the frequencies of each component. We assume that for all $j\in\{1,\dots,J\}$:
\begin{equation}
\exists\, p_j,p_j'\in\NN^*: \quad f_j = \dfrac{p_j}{M}\fs = \dfrac{p'_j}{K}\fs\ .
\end{equation}
\end{remark}

In addition, the observed signal is assumed to be corrupted by an additive Gaussian white noise. Therefore, the measured discrete signal $\bx$ is written as:
\begin{equation}
\bx = \bz + \sigma\bw\ ,
\label{eq:model.noise}
\end{equation}
where $\bz$ follows model~\eqref{eq:sum.sine}, $\bw$ is a Gaussian white noise, whose variance is normalized to one. Thus, $\sigma^2$ denotes the variance of the additive noise $\sigma\bw$.

\subsection{Forecasting error}
On the forecasting interval, we decompose the estimated signal $\tilde\bx$ as follows:
\begin{equation}
\tilde \bx[n] = \bz[n] + \bepsilon[n]\ ,
\label{eq:forecasting.error}
\end{equation}
where $\bepsilon$ is a the forecasting error. When $n\in I=\{0,\dots,N-1\}$, this error is only containing the measurement noise, that is $\bepsilon[n]=\sigma\bw[n]$. Outside the interval $I$, the importance of the forecasting error $\bepsilon$ is also affected by the loss of information resulting from the linearization of dynamical model we consider in~\eqref{eq:dyn.model}. To evaluate the actual behavior of the forward forecasting error $\bepsilon[n]$ when $n\geq N$, we determine its two first moments.
\begin{enumerate}
\item 
The mean, which is also the estimation bias, is such that: 
\begin{align*}
\bmu[n]&\defeq\EE\{\bepsilon[n]\} \\
&=\EE\{\tilde\bx[n]\} - \bz[n]\ .
\end{align*}
Given the forecasting strategy, we have $\bmu[n]=0$ when $n\in I$ and: 
\begin{equation}
\bmu[n]= \EE\{\balpha^{(\ell)}\}\bz_{K} + \sigma\EE\{\balpha^{(n-N+1)}\bw_{K}\} - \bz[n]
\label{eq:bias0}
\end{equation}
when $n\geq N$.
\item 
The covariance is given by:
\begin{align*}
\bgamma[n,n'] &\defeq \EE\{\left(\bepsilon[n]-\bmu[n]\right)\left(\bepsilon[n']-\bmu[n']\right)\}\\
&= \EE\{\tilde\bx[n]\tilde\bx[n']\}-\bz[n]\bz[n'] -\bmu[n]\bz[n'] \\
&\hspace{15pt} - \bmu[n']\bz[n] -\bmu[n]\bmu[n']\ .
\end{align*}
Thus by definition of the noise, we have $\bgamma[n,n'] =\sigma^2\delta_{n,n'}$ when $(n,n')\in I^2$. When $n\geq N$, let us denote $\ell=n-N+1$. Then, we have two cases.
\begin{enumerate}[label=(\roman*)]
\item If $n'\in I$:
\begin{align}
\nonumber
\bgamma[n,n'] &= \sigma\EE\{\bw[n']\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\bw[n']\balpha^{(\ell)}\bw_K\} \\
&\hspace{15pt} -\bz[n]\bmu[n'] - \bmu[n]\bmu[n']\ .
\label{eq:cov00}
\end{align}
\item If $n'=N-1+\lambda\geq N$:
\begin{align}
\nonumber
\hspace{-6pt}\bgamma[n,n'] &= \bz_K^T\EE\left\{{\balpha^{(\ell)}}^T\balpha^{(\lambda)}\right\}\bz_K + \sigma\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\}\bz_K \\
\nonumber
&\hspace{-3pt} + \sigma\EE\{\balpha^{(\lambda)}\bw_K\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\bw_K\}  \\
&\hspace{-3pt} -\!\bz[n]\bz[n']-\!\bz[n]\bmu[n']-\!\bz[n]\bmu[n'] -\! \bmu[n]\bmu[n']\,.
\label{eq:cov01}
\end{align}
\end{enumerate}
Besides, we recall that $\bgamma[n,n']=\bgamma[n',n]$.
\end{enumerate}

Expressions~\eqref{eq:bias0},~\eqref{eq:cov00}, and~\eqref{eq:cov01} show that these quantities depend on the behavior of the forecasting random vector $\balpha^{(\ell)}$. In Lemma~\ref{lm:error}, we specify the asymptotic behavior of the forecasting vector $\balpha^{(\ell)}$ when the dataset size $K$ is great.
\begin{lemma}
\label{lm:error}
Let $\bx$ be a random vector defined by~\eqref{eq:model.noise}. Let $\balpha^{(\ell)}$ be the associated forecasting vector for the estimation of $\bx[N-1+\ell]$, given by~\eqref{eq:alpha.l} and obtained from the least square estimation~\eqref{eq:lse}. Let $\balpha_0^{(\ell)}$ be the last row of the matrix $\bA_0^\ell$, where $\bA_0$ is defined by:
\begin{equation}
\bA_0 = \left(\dfrac1K\bZ'\bZ^T+\sigma^2\bD\right)\left(\dfrac1K\bZ\bZ^T+\sigma^2\bI\right)^{\inv}\ ,
\label{eq:lse.0}
\end{equation}
with $\bZ=\begin{pmatrix} \bz_0 & \cdots & \bz_{K-1}\end{pmatrix}$ and $\bZ'=\begin{pmatrix} \bz_1 & \cdots & \bz_{K}\end{pmatrix}$, where $\bz_k$ is the $k$-th sub-signal extracted from $\bz$ in the same way as $\bx_k$ is defined from $\bx$ in~\eqref{eq:xk}. Besides, $\bD\in\RR^{M\times M}$ is the Toeplitz matrix such that:
\[
\bD[m,m'] = \delta_{m+1,m'}\ .
\]
Let $\bh^{(\ell)}$ be the error vector given by:
\begin{equation*}
\bh^{(\ell)} = \balpha^{(\ell)} - \balpha_0^{(\ell)}\ .
\end{equation*}
Then, the random vector $\bh^{(\ell)}$ converges in law to a zero-mean Gaussian random vector when $K\to\infty$, and we have:
\begin{equation}
\sqrt{K}\,\bh^{(\ell)}  \xrightarrow[K\to\infty]{\cD} \cN\left(\bzero,\bGamma^{(\ell,\ell)}\right)\ ,
\end{equation}
with $\bGamma^{(\ell,\ell)}={\bF^{(\ell)}}^T\bGamma_0\bF^{(\ell)}$, where $\bGamma_0\in\RR^{M(M+1)\times M(M+1)}$ is a covariance matrix and $\bF^{(\ell)}\in\RR^{M(M+1)\times M}$ is a Jacobian matrix. The expressions of these two matrices do not depend on $K$ or $\sigma$. 
% such that:
%{\color{red}
%\begin{align*}
%\bGamma_0[mM+n,pM+q] &= 2\sigma^2\sum_{j=1}^J\Omega_j^2\cos\left(2\pi\frac{f_j}{\fs}(m-n)\right)\\
%& \hspace{26pt} \times\cos\left(2\pi\frac{f_j}{\fs}(p-q)\right) + o(\sigma^2) \\
%\bF^{(\ell)}[] &= \ . 
%\end{align*}
%}
\end{lemma}

\begin{proof}
See the Supplementary Material. The proof is based on the multivariate delta method (see paragraph 7.2 in~\cite{Alho05statistical}), which allows to asymptotically approximate a random vector normal as a Gaussian random vector.
\end{proof}

Consequently, the covariance between $\sqrt{K}\bh^{(\ell)}$ and $\sqrt{K}\bh^{(\lambda)}$ remains bounded, \ie:
\begin{equation*}
K\,\EE\left\{{\bh^{(\ell)}}^T\bh^{(\lambda)}\right\} \xrightarrow[K\to\infty]{} \bGamma^{(\ell,\lambda)} = {\bF^{(\ell)}}^T\bGamma_0\bF^{(\lambda)}\ .
\end{equation*}

\begin{theorem}
\label{th:error}
Let $\bx\in\RR^N$ be a discrete-time random signal following model~\eqref{eq:model.noise}. Let $\tilde\bx$ denotes its forecasting, obtained using the extension Algorithm~\ref{alg:extension}. Let $n\geq N$ be a sample index. Then, the first-order moment of the forecasting error $\epsilon[n]$ in~\eqref{eq:forecasting.error} is approximated by:
\begin{equation}
\bmu[n] \underset{K\to\infty}{\sim} o(\sigma^2)
\label{eq:mean.error}
\end{equation}
Its second-order moment $\bgamma[n,n']$ verify the following approximation equations:
\begin{enumerate}[label=(\roman*)]
\item if $n'\in I=\{0,\ldots,N-1\}$:
\begin{equation}
\bgamma[n,n']\underset{K\to\infty}{\sim} \sigma^2\balpha_0^{(n-N-1)}[n'-(N-M)]\1_{(n'\geq N-M)}
\label{eq:cov.error.1}
\end{equation}
\item if $n'\geq N$:
\begin{equation}
\hspace{-15pt}\bgamma[n,n']\!\underset{K\to\infty}{\sim}\!\frac1K\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K \!+\! \dfrac{\sigma^2}{K}\Tr\left(\bGamma^{(\ell,\lambda)}\!\right)\! + \sigma^2\!\left\langle\balpha_0^{(\ell)}\!, \balpha_0^{(\lambda)}\right\rangle ,
\label{eq:cov.error.2}
\end{equation}
where $\ell=n-N+1$ and $\lambda=n'-N+1$.
\end{enumerate}
\end{theorem}

\begin{proof}
See the Supplementary Material. The proof is mainly based on the results provided by Lemma~\ref{lm:error}, combined with the Isserlis' theorem~\cite{Isserlis16formula}, which provides a formula for the computation of higher-order moments of Gaussian random variables.
\end{proof}

Ideally, the forecasting error would behave like the measurement noise $\sigma\bw$, \ie~a zero-mean noise whose variance is of the order of $\sigma^2$. Theorem~\ref{th:error} shows that the forecasting error is asymptotically unbiased. Concerning the covariance of the forecasting error, although equations~\eqref{eq:cov.error.1} and~\eqref{eq:cov.error.2} are not easily readable, one can evaluate the dependence of the variance in function of the tuning parameters, that are adjusted by the user. Let us focus on the forecasting error variance $\gamma[n,n]$ when $n\geq N$. First, as expected, the variance increases linearly with the noise variance $\sigma^2$. Second, it asymptotically depends linearly on the ratio $\frac{1}{K}$. This shows the need to use a sufficiently large dataset to obtain an accurate forecast. Third, the dependency on the sub-signals lengths $M$ and the forecasting index $\ell=n-N+1$ is hidden in the expression of the covariance matrix $\bGamma^{(\ell,\ell)}$. We discuss this dependency in more detail in section~\ref{ssse:res.sine}.

\begin{remark}[AM-FM Model]
One can extend the previous result to the case where the instantaneous frequencies and amplitudes of the components of the deterministic part of the observed signal are slowly varying. We therefore handle the \textit{AM-FM model} which, in its is continuous-time version, takes the following form:
\begin{equation}
z(t) = \sum_{j=1}^J a_j(t)\cos(2\pi\phi_j(t))\ ,
\end{equation}
where $a_j$ and $\phi'_j$ are smooth function. These 
restrictions fixed on the variations of the instantaneous amplitudes and frequencies are specified by the so-called adaptive harmonic model (see~\cite{Daubechies16conceft} for example). In the case, the forecasting error is additionally sensitive to the speed of variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$. 
\end{remark}


\subsection{Performance of the boundary effects reduction}
\label{sse:perf.BoundEffRed}
Since it is beyond the scope of the present paper, we do not provide here a generic proof of the reduction of the boundary effects on any time-frequency representation. Let us focus instead on the particular case of synchrosqueezing transform. Since we analyze AM-FM signals, Theorem~\ref{th:error} ensures that the forecasting error $\bepsilon$ defined in~\eqref{eq:forecasting.error} is controlled and bounded in terms of mean and covariance. We can then rely on Theorem 3 in~\cite{Chen14nonparametric}. Indeed, this theorem states that when the additive noise remains small, the SST of the observed signal remains close to the ideal SST of an AM-FM signal, throughout the time-frequency plane.  We refer the reader to~\cite{Chen14nonparametric} for a precise quantification of the error made in the time-frequency plane, which depends notably on the covariance of the additive noise and on the speed of variation of the amplitudes and instantaneous frequencies composing the signal. Therefore, in our case, this means that boundary effects are strongly reduced since instantaneous frequencies can now be approximately observed continuously up to the edges of the time-frequency plane.

The extension of this result to other time-frequency representations is not provided theoretically. In the following section~\ref{se:results}, we verify experimentally that algorithm~\ref{alg:boundary} is efficient for a large number of representations.