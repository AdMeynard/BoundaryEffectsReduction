\subsection{Signal model}
We model the deterministic part of the observed signal as a multicomponent harmonic signal, that is a sum of sine waves. Then:
\begin{equation}
\bz[n]=\sum_{j=1}^J\Omega_j\cos\left(2\pi f_j \frac{n}{\fs}\right)\ ,
\label{eq:sum.sine}
\end{equation}
where $J$ denotes the number of components, $\Omega_j$ the amplitude of the $j$-th component, and $f_j$ its frequency.

\begin{remark}
For the sake of simplicity, we make an additional assumption on the frequencies of each component. We assume that for all $j\in\{1,\dots,J\}$:
\begin{equation}
\exists\, p_j,p_j'\in\NN^*: \quad f_j = \dfrac{p_j}{M}\fs = \dfrac{p'_j}{K}\fs\ .
\end{equation}
\end{remark}

In addition, the observed signal is assumed to be corrupted by an additive Gaussian white noise. Therefore, the measured discrete signal $\bx$ is written as:
\begin{equation}
\bx = \bz + \sigma\bw\ ,
\label{eq:model.noise}
\end{equation}
where $\bz$ follows model~\eqref{eq:sum.sine}, $\bw$ is a Gaussian white noise, whose variance is normalized to one. Thus, $\sigma^2$ denotes the variance of the additive noise $\sigma\bw$.

\subsection{Forecasting error}
On the forecasting interval, we decompose the estimated signal $\tilde\bx$ as follows:
\begin{equation}
\tilde \bx[n] = \bz[n] + \bepsilon[n]\ ,
\label{eq:forecasting.error}
\end{equation}
where $\bepsilon$ is a the forecasting error. When $n\in I=\{0,\dots,N-1\}$, this error is only containing the measurement noise, that is $\bepsilon[n]=\sigma\bw[n]$. Outside the interval $I$, the importance of the forecasting error $\bepsilon$ is also affected by the loss of information resulting from the linearization of dynamical model we consider in~\eqref{eq:dyn.model}. To evaluate the actual behavior of the forward forecasting error $\bepsilon[n]$ when $n\geq N$, we determine its two first moments.
\begin{enumerate}
\item 
The mean, which is also the estimation bias, is such that: 
\begin{align*}
\bmu[n]&\defeq\EE\{\bepsilon[n]\} \\
&=\EE\{\tilde\bx[n]\} - \bz[n]\ .
\end{align*}
Given the forecasting strategy, we have $\bmu[n]=0$ when $n\in I$ and: 
\begin{equation}
\bmu[n]= \EE\{\balpha^{(\ell)}\}\bz_{K} + \sigma\EE\{\balpha^{(n-N+1)}\bw_{K}\} - \bz[n]
\label{eq:bias0}
\end{equation}
when $n\geq N$.
\item 
The covariance is given by:
\begin{align*}
\bgamma[n,n'] &\defeq \EE\{\left(\bepsilon[n]-\bmu[n]\right)\left(\bepsilon[n']-\bmu[n']\right)\}\\
&= \EE\{\tilde\bx[n]\tilde\bx[n']\}-\bz[n]\bz[n'] -\bmu[n]\bz[n'] \\
&\hspace{15pt} - \bmu[n']\bz[n] -\bmu[n]\bmu[n']\ .
\end{align*}
Thus by definition of the noise, we have $\bgamma[n,n'] =\sigma^2\delta_{n,n'}$ when $(n,n')\in I^2$. When $n\geq N$, let us denote $\ell=n-N+1$. Then, we have two cases.
\begin{enumerate}[label=(\roman*)]
\item If $n'\in I$:
\begin{align*}
\bgamma[n,n'] &= \sigma\EE\{\bw[n']\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\bw[n']\balpha^{(\ell)}\bw_K\} \\
&\hspace{15pt} -\bz[n]\bmu[n'] - \bmu[n]\bmu[n']\ .
\end{align*}
\item If $n'=N-1+\lambda\geq N$:
\begin{align}
\nonumber
\hspace{-6pt}\bgamma[n,n'] &= \bz_K^T\EE\left\{{\balpha^{(\ell)}}^T\balpha^{(\lambda)}\right\}\bz_K + \sigma\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\}\bz_K \\
\nonumber
&\hspace{-3pt} + \sigma\EE\{\balpha^{(\lambda)}\bw_K\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\bw_K\}  \\
&\hspace{-3pt} -\!\bz[n]\bz[n']-\!\bz[n]\bmu[n']-\!\bz[n]\bmu[n'] -\! \bmu[n]\bmu[n']\,.
\label{eq:cov0}
\end{align}
\end{enumerate}
Besides, we recall that $\bgamma[n,n']=\bgamma[n',n]$.
\end{enumerate}

Expressions~\eqref{eq:bias0} and~\eqref{eq:cov0} show that these quantities depend on the behavior of the forecasting random vector $\balpha^{(\ell)}$. First, notice that the forecasting matrix $\tilde\bA$ would ideally take the form:
\begin{equation}
\bA_0 = \left(\dfrac1K\bZ'\bZ^T+\sigma^2\bD\right)\left(\dfrac1K\bZ\bZ^T+\sigma^2\bI\right)^{\inv}\ ,
\label{eq:lse.0}
\end{equation}
with $\bZ=\begin{pmatrix} \bz_0 & \cdots & \bz_{K-1}\end{pmatrix}$ and $\bZ'=\begin{pmatrix} \bz_1 & \cdots & \bz_{K}\end{pmatrix}$, where $\bz_k$ is the $k$-th sub-signal extracted from $\bz$ in the same way as $\bx_k$ is defined from $\bx$ in~\eqref{eq:xk}. Besides, $\bD\in\RR^{M\times M}$ is the Toeplitz matrix such that:
\[
\bD[m,m'] = \delta_{m+1,m'}\ .
\]
Lemma~\ref{lm:error} specify the asymptotic behavior of the forecasting vector $\balpha^{(\ell)}$ when the dataset size $K$ is great.
\begin{lemma}
\label{lm:error}
Let $\bx$ be a random vector defined by~\eqref{eq:model.noise}. Let $\balpha^{(\ell)}$ be the associated forecasting vector for the estimation of $\bx[N-1+\ell]$, given by~\eqref{eq:alpha.l} and obtained from the least square estimation~\eqref{eq:lse}. Let $\balpha_0^{(\ell)}$ be the last row of the ideal forecasting matrix $\bA_0^\ell$. Let $\bh^{(\ell)}$ be the error vector given by:
\begin{equation*}
\bh^{(\ell)} = \balpha^{(\ell)} - \balpha_0^{(\ell)}\ .
\end{equation*}
Then, the random vector $\bh^{(\ell)}$ converges in law to a zero-mean Gaussian random vector when $K\to\infty$, and we have:
\begin{equation}
\sqrt{K}\,\bh^{(\ell)}  \xrightarrow[K\to\infty]{\cD} \cN\left(\bzero,\bGamma^{(\ell,\ell)}\right)\ ,
\end{equation}
with $\bGamma^{(\ell,\ell)}={\bF^{(\ell)}}^T\bGamma_0\bF^{(\ell)}$, where $\bGamma_0\in\RR^{M(M+1)\times M(M+1)}$ is a covariance matrix and $\bF^{(\ell)}\in\RR^{M(M+1)\times M}$ is a Jacobian matrix. The expressions of these two matrices do not depend on $K$ or $\sigma$. 
% such that:
%{\color{red}
%\begin{align*}
%\bGamma_0[mM+n,pM+q] &= 2\sigma^2\sum_{j=1}^J\Omega_j^2\cos\left(2\pi\frac{f_j}{\fs}(m-n)\right)\\
%& \hspace{26pt} \times\cos\left(2\pi\frac{f_j}{\fs}(p-q)\right) + o(\sigma^2) \\
%\bF^{(\ell)}[] &= \ . 
%\end{align*}
%}
\end{lemma}

\begin{proof}
See the Supplementary Material. The proof is based on the multivariate delta method (see paragraph 7.2 in~\cite{Alho05statistical}), which allows to asymptotically approximate a random vector normal as a Gaussian random vector.
\end{proof}

Consequently, the covariance between $\sqrt{K}\bh^{(\ell)}$ and $\sqrt{K}\bh^{(\lambda)}$ remains bounded, \ie:
\begin{equation*}
K\,\EE\left\{{\bh^{(\ell)}}^T\bh^{(\lambda)}\right\} \xrightarrow[K\to\infty]{} \bGamma^{(\ell,\lambda)} = {\bF^{(\ell)}}^T\bGamma_0\bF^{(\lambda)}\ .
\end{equation*}

\begin{theorem}
\label{th:error}
Let $\bx\in\RR^N$ be a discrete-time random signal following model~\eqref{eq:model.noise}. Let $\tilde\bx$ denotes its forecasting, obtained using the extension Algorithm~\ref{alg:extension}. Let $n\geq N$ be a sample index. Then, the first-order moment of the forecasting error $\epsilon[n]$ in~\eqref{eq:forecasting.error} is approximated by:
\begin{equation}
\bmu[n] \underset{K\to\infty}{\sim} o(\sigma^2)
\label{eq:mean.error}
\end{equation}
Its second-order moment $\bgamma[n,n']$ verify the following approximation equations:
\begin{enumerate}[label=(\roman*)]
\item if $n'\in I=\{0,\ldots,N-1\}$:
\begin{equation}
\bgamma[n,n']\underset{K\to\infty}{\sim} \sigma^2\balpha_0^{(n-N-1)}[m-(N-M)]\1_{(m\geq N-M)}
\label{eq:cov.error.1}
\end{equation}
\item if $n'\geq N$:
\begin{equation}
\hspace{-15pt}\bgamma[n,n']\!\underset{K\to\infty}{\sim}\!\frac1K\bz_K^T\bGamma^{(l,\lambda)}\bz_K +\! \dfrac{\sigma^2}{K}\Tr\left(\bGamma^{(\ell,\lambda)}\!\right)\! + \sigma^2\!\left\langle\balpha_0^{(\ell)}\!, \balpha_0^{(\lambda)}\right\rangle ,
\label{eq:cov.error.2}
\end{equation}
where $\ell=n-N+1$ and $\lambda=n'-N+1$.
\end{enumerate}
\end{theorem}

\begin{proof}
See the Supplementary Material. The proof is mainly based on the results provided by Lemma~\ref{lm:error}, combined with the Isserlis' theorem~\cite{Isserlis16formula}, which provides a formula for the computation of higher-order moments of Gaussian random variables.
\end{proof}

Ideally, the forecasting error would behave like the measurement noise $\sigma\bw$, \ie~a zero-mean noise whose variance is of the order of $\sigma^2$. Theorem~\ref{th:error} shows that the forecasting error is asymptotically unbiased. Concerning the covariance of the forecasting error, although equations~\eqref{eq:cov.error.1} and~\eqref{eq:cov.error.2} are not easily readable, one can evaluate the dependence of the variance in function of the tuning parameters, that are adjusted by the user. Let us focus on the forecasting error variance $\gamma[n,n]$ when $n\geq N$. First, as expected, the variance increases linearly with the noise variance $\sigma^2$. Second, it asymptotically depends linearly on the ratio $\frac{1}{K}$. This shows the need to use a sufficiently large dataset to obtain an accurate forecast. Third, the dependency on the sub-signals lengths $M$ and the forecasting index $\ell=n-N+1$ is hidden in the expression of the covariance matrix $\bGamma^{(\ell,\ell)}$. We discuss this dependency in more detail in section~\ref{ssse:res.sine}.

\begin{remark}[Adaptive Harmonic Model]
One can extend the previous result to the case where the instantaneous frequencies and amplitudes of the components of the deterministic part of the observed signal are slowly varying. We therefore handle the \textit{adaptive harmonic model} which, in its is continuous-time version, takes the following form:
\begin{equation}
z(t) = \sum_{j=1}^J a_j(t)\cos(2\pi\phi_j(t))\ ,
\end{equation}
where $a_j$ and $\phi'_j$ are smooth function. In the case, the forecasting error is additionally sensitive to the speed of variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$. 
\end{remark}

{\color{red}
\subsection{Performance of the boundary effects reduction}
}