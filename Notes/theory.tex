\subsection{Signal Model}
We model the deterministic part of the observed signal as a multicomponent harmonic signal; that is, a sum of sine waves:
\begin{equation}
\bz[n]=\sum_{j=1}^J\Omega_j\cos\left(2\pi f_j \frac{n}{\fs} + \varphi_j\right)\ ,
\label{eq:sum.sine}
\end{equation}
where $J$ denotes the number of components, $\Omega_j>0$ the amplitude of the $j$-th component, $f_j$ its frequency, and $\varphi_j\in[0,2\pi)$ its initial phase.
%
For the sake of simplicity, we make an additional assumption on the frequencies of each component. We assume that for all $j\in\{1,\dots,J\}$:
\begin{equation}
\exists\, p_j,p_j'\in\NN^*: \quad f_j = \dfrac{p_j}{M}\fs = \dfrac{p'_j}{K}\fs\ .
\end{equation}


In addition, the observed signal is assumed to be corrupted by an additive Gaussian white noise. Therefore, the measured discrete signal $\bx$ is written as:
\begin{equation}
\bx = \bz + \sigma\bw\ ,
\label{eq:model.noise}
\end{equation}
where $\bz$ follows model~\eqref{eq:sum.sine}, $\bw$ is a Gaussian white noise, whose variance is normalized to one. Thus, $\sigma^2$ denotes the variance of the additive noise $\sigma\bw$.

\subsection{Forecasting Error}
On the forecasting interval, we decompose the estimated signal $\tilde\bx$ as follows:
\begin{equation}
\tilde \bx[n] = \bz[n] + \bepsilon[n]\ ,
\label{eq:forecasting.error}
\end{equation}
where $\bepsilon$ is the forecasting error. When $n\in I=\{0,\dots,N-1\}$, this error contains only the measurement noise, that is $\bepsilon[n]=\sigma\bw[n]$. Outside the interval $I$, the importance of the forecasting error $\bepsilon$ is also affected by the loss of information resulting from the linearization of the dynamical model we consider in~\eqref{eq:dyn.model}. To evaluate the actual behavior of the forward forecasting error $\bepsilon[n]$ when $n\geq N$, we determine its first two moments.
\begin{enumerate}
\item 
The mean, or estimation bias, is such that: 
\begin{align}
\bmu[n]&\defeq\EE\{\bepsilon[n]\} 
=\EE\{\tilde\bx[n]\} - \bz[n]\ .\label{eq:defn:bias0}
\end{align}
Given the forecasting strategy, we have $\bmu[n]=0$ when $n\in I$ and
\begin{equation}
\bmu[n]= \EE\{\balpha^{(\ell)}\}\bz_{K} + \sigma\EE\{\balpha^{(n-N+1)}\bw_{K}\} - \bz[n]
\label{eq:bias0}
\end{equation}
when $n\geq N$.
\item 
The covariance is given by:
\begin{align*}
\bgamma[n,n'] &\defeq \EE\{\left(\bepsilon[n]-\bmu[n]\right)\left(\bepsilon[n']-\bmu[n']\right)\}\\
&= \EE\{\tilde\bx[n]\tilde\bx[n']\}-\bz[n]\bz[n'] -\bmu[n]\bz[n'] \\
&\hspace{15pt} - \bmu[n']\bz[n] -\bmu[n]\bmu[n']\ .
\end{align*}
Thus by definition of the noise, we have $\bgamma[n,n'] =\sigma^2\delta_{n,n'}$ when $(n,n')\in I^2$. When $n\geq N$, let us denote $\ell=n-N+1$. Then, we have two cases.
\begin{enumerate}[label=(\roman*)]
\item If $n'\in I$:
\begin{align}
\nonumber
\bgamma[n,n'] &= \sigma\EE\{\bw[n']\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\bw[n']\balpha^{(\ell)}\bw_K\} \\
&\hspace{15pt} -\bz[n]\bmu[n'] - \bmu[n]\bmu[n']\ .
\label{eq:cov00}
\end{align}
\item If $n'=N-1+\lambda\geq N$:
\begin{align}
\nonumber
\hspace{-6pt}\bgamma[n,n'] &= \bz_K^T\EE\left\{{\balpha^{(\ell)}}^T\balpha^{(\lambda)}\right\}\bz_K + \sigma\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\}\bz_K \\
\nonumber
&\hspace{-3pt} + \sigma\EE\{\balpha^{(\lambda)}\bw_K\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\bw_K\}  \\
&\hspace{-3pt} -\!\bz[n]\bz[n']-\!\bz[n]\bmu[n']-\!\bz[n]\bmu[n'] -\! \bmu[n]\bmu[n']\,.
\label{eq:cov01}
\end{align}
\end{enumerate}
Besides, we recall that $\bgamma[n,n']=\bgamma[n',n]$.
\end{enumerate}

Expressions~\eqref{eq:bias0},~\eqref{eq:cov00}, and~\eqref{eq:cov01} show that these quantities depend on the behavior of the forecasting random vector $\balpha^{(\ell)}$. In Lemma~\ref{lm:error}, we specify the asymptotic behavior of the forecasting vector $\balpha^{(\ell)}$ when the dataset size $K$ is great.
\begin{lemma}
\label{lm:error}
Let $\bx$ be a random vector defined by~\eqref{eq:model.noise}. Let $\balpha^{(\ell)}$ be the associated forecasting vector for the estimation of $\bx[N-1+\ell]$, given by~\eqref{eq:alpha.l} and obtained from the least square estimation~\eqref{eq:lse}. Let $\balpha_0^{(\ell)}$ be the last row of the matrix $\bA_0^\ell$, where $\bA_0$ is defined by:
\begin{equation}
\bA_0 = \left(\dfrac1K\bZ'\bZ^T+\sigma^2\bD\right)\left(\dfrac1K\bZ\bZ^T+\sigma^2\bI\right)^{\inv}\ ,
\label{eq:lse.0}
\end{equation}
where $\bZ=\begin{pmatrix} \bz_0 & \cdots & \bz_{K-1}\end{pmatrix}$, $\bZ'=\begin{pmatrix} \bz_1 & \cdots & \bz_{K}\end{pmatrix}$, $\bz_k$ is the $k$-th subsignal extracted from $\bz$ in the same way as $\bx_k$ is defined from $\bx$ in~\eqref{eq:xk}, and $\bD\in\RR^{M\times M}$ is the Toeplitz matrix such that $\bD[m,m'] = \delta_{m+1,m'}$.

Let $\bh^{(\ell)}$ be the error vector given by
\begin{equation*}
\bh^{(\ell)} = \balpha^{(\ell)} - \balpha_0^{(\ell)}\, .
\end{equation*}
Then, the random vector $\bh^{(\ell)}$ converges in law to a zero-mean Gaussian random vector when $K\to\infty$; that is,
\begin{equation}
\sqrt{K}\,\bh^{(\ell)}  \xrightarrow[K\to\infty]{\cD} \cN\left(\bzero,\bGamma^{(\ell,\ell)}\right)\ ,
\end{equation}
with $\bGamma^{(\ell,\ell)}:={\bF^{(\ell)}}^T\bGamma_0\bF^{(\ell)}$, where $\bGamma_0\in\RR^{M(M+1)\times M(M+1)}$ is a covariance matrix and $\bF^{(\ell)}\in\RR^{M(M+1)\times M}$ is a Jacobian matrix, and $\bGamma^{(\ell,\ell)}$ does not depend on $\sigma$. 
% such that:
%{\color{red}
%\begin{align*}
%\bGamma_0[mM+n,pM+q] &= 2\sigma^2\sum_{j=1}^J\Omega_j^2\cos\left(2\pi\frac{f_j}{\fs}(m-n)\right)\\
%& \hspace{26pt} \times\cos\left(2\pi\frac{f_j}{\fs}(p-q)\right) + o(\sigma^2) \\
%\bF^{(\ell)}[] &= \ . 
%\end{align*}
%}
\end{lemma}

\begin{proof}
See the Supplementary Material. The proof is based on the multivariate delta method (see paragraph 7.2 in~\cite{Alho05statistical}), which gives an asymptotic approximation of a random vector by a Gaussian random vector.
\end{proof}

Consequently, the covariance between $\sqrt{K}\bh^{(\ell)}$ and $\sqrt{K}\bh^{(\lambda)}$ remains bounded, \ie:
\begin{equation*}
K\,\EE\left\{{\bh^{(\ell)}}^T\bh^{(\lambda)}\right\} \xrightarrow[K\to\infty]{} \bGamma^{(\ell,\lambda)} = {\bF^{(\ell)}}^T\bGamma_0\bF^{(\lambda)}\ .
\end{equation*}

\begin{theorem}
\label{th:error}
Let $\bx\in\RR^N$ be a discrete-time random signal following model~\eqref{eq:model.noise}. Let $\tilde\bx$ denotes its forecasting, obtained using the extension Algorithm~\ref{alg:extension}. Let $n\geq N$ be a sample index. Then, the first-order moment of the forecasting error $\epsilon[n]$ in~\eqref{eq:forecasting.error} defined in \eqref{eq:defn:bias0} satisfies
\begin{equation}
\frac{1}{\sigma}\bmu[n] = o(1)
\label{eq:mean.error}
\end{equation}
when $K\to\infty$.
Its second-order moment $\bgamma[n,n']$ satisfies the following approximation equations:
\begin{enumerate}[label=(\roman*)]
\item if $n'\in I=\{0,\ldots,N-1\}$:
\begin{equation}
\bgamma[n,n']\to \sigma^2\balpha_0^{(n-N-1)}[n'-(N-M)]\1_{(n'\geq N-M)}
\label{eq:cov.error.1}
\end{equation}
when $K\to\infty$;
\item if $n'\geq N$:
\begin{equation}
\hspace{-15pt}\bgamma[n,n'] = \dfrac1K\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K + \dfrac{\sigma^2}{K}\Tr\left(\bGamma^{(\ell,\lambda)}\right) + \sigma^2\left\langle\balpha_0^{(\ell)}\!, \balpha_0^{(\lambda)}\right\rangle ,
\label{eq:cov.error.2}
\end{equation}
when $K\to\infty$, where $\ell\!=n-N+1$ and $\lambda\!=n'-N+1$.
\end{enumerate}
\end{theorem}

\begin{proof}
See the Supplementary Material. The proof is mainly based on the results provided by Lemma~\ref{lm:error}, combined with the Isserlis' theorem~\cite{Isserlis16formula}, which provides a formula for the computation of higher-order moments of Gaussian random variables.
\end{proof}

Ideally, the forecasting error would behave like the measurement noise $\sigma\bw$, \ie~a zero-mean noise whose variance is of the order of $\sigma^2$. Theorem~\ref{th:error} shows that the forecasting error is asymptotically unbiased. Concerning the covariance of the forecasting error, although equations~\eqref{eq:cov.error.1} and~\eqref{eq:cov.error.2} are not easily readable, one can evaluate the dependence of the variance in function of the tuning parameters, that are adjusted by the user. Let us focus on the forecasting error variance $\gamma[n,n]$ when $n\geq N$. First, as expected, the variance increases linearly with the noise variance $\sigma^2$. Second, it asymptotically depends linearly on the ratio $\frac{1}{K}$. This shows the need to use a sufficiently large dataset to obtain an accurate forecast. Third, the dependency on the subsignals lengths $M$ and the forecasting index $\ell=n-N+1$ is hidden in the expression of the covariance matrix $\bGamma^{(\ell,\ell)}$. We discuss this dependency in more detail in section~\ref{ssse:res.sine}.

\subsection{Adaptive Harmonic Model}\label{RemarkAHM}

One can extend the previous result to the case where the instantaneous frequencies and amplitudes of the components of the deterministic part of the observed signal are {\em slowly varying}. It is an \textit{AM-FM model} that, in its is continuous-time version, takes the following form
\begin{equation}
z(t) = \sum_{j=1}^J a_j(t)\cos(2\pi\phi_j(t))\,,
\end{equation}
where $a_j(t)$ and $\phi'_j(t)$ describe how large and fast the signal oscillate at time $t$. 
Clearly, \eqref{eq:sum.sine} is a special case satisfying the AM-FM model when the $a_j$ and $\phi'_j$ are both constants. 
%
In many practical signals, the amplitude and frequency do not change fast. It is thus reasonable to further restrict the regularity and variations of the instantaneous amplitudes and frequencies of these AM-FM functions. When the speeds of variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$ are small, the signal can be ``locally'' well approximated by a harmonic function in \eqref{eq:sum.sine}; that is, 
%
locally at time $t_0$, $z(t)$ can be well approximated by $z_0(t) := \sum_{j=1}^J a_j(t_0)\cos(2\pi(\phi_j(t_0)-t_0\phi'_j(t_0)+\phi'_j(t_0)t))$ by the Taylor expansion. An AM-FM function satisfying the slow variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$ is referred to the adaptive harmonic model (AHM) (see~\cite{Chen14nonparametric,Daubechies16conceft} for mathematical details).
It is thus clear that the forecasting error caused by the {\sf SigExt} algorithm additionally depends on the speed of variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$. 
%
For the forecasting purpose, it is thus clear that when $K$ is not too large, the signal can be well approximated by \eqref{eq:sum.sine}. Hence, Theorem \ref{th:error} can still be applied to approximate the error. However, how to determine the optimal $K$ based on the signal depends on the application and is out of the scope of this paper. It will be explored in future work for specific application problems.
%\end{remark}


\subsection{Performance of the Boundary Effects Reduction}
\label{sse:perf.BoundEffRed}
While we do not provide a generic proof of the boundary effect reduction on {\em any} TF analysis tools, we discuss a particular case of SST. Since SST is designed to analyze signals satisfying the AHM, as is discussed in Remark \ref{RemarkAHM}, Theorem~\ref{th:error} ensures that the forecasting error $\bepsilon$ defined in~\eqref{eq:forecasting.error} is controlled and bounded in terms of mean and covariance. Recall Theorem 3 in~\cite{Chen14nonparametric}, which states that when the additive noise is stationary and maybe modulated by a smooth function with a small fluctuation, the SST of the observed signal remains close to the SST of the clean signal, throughout the TF plane. 
%
We refer the reader to~\cite{Chen14nonparametric} for a precise quantification of the error made in the TF plane, which depends notably on the covariance of the additive noise and on the speed of variation of the amplitudes and instantaneous frequencies composing the signal. 
%
In our case, while the noise of the historical data is stationary, the forecasting error depends on the historical noise, and hence the overall ``noise'' is non-stationary. However, the dependence only appears in the extended part of the signal. We claim that the proof of Theorem 3 in~\cite{Chen14nonparametric} can be generalized to explain the robustness of SST to the noise, and a systematic proof will be reported in our future theoretical work. While we do not provide a theoretical justification, we verify experimentally that Algorithm~\ref{alg:boundary} is efficient for a large number of representations in the following section~\ref{se:results}. 
%
Therefore, in our case, this means that the boundary effect is strongly reduced since the impact of the forecasting error does SST is limited. An immediate application is that the instantaneous frequencies can now be well estimated continuously up to the edges of the TF plane, and a real-time acquisition of the instantaneous frequency and amplitude information is possible.