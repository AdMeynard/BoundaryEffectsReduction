\documentclass[journal,onecolumn]{IEEEtran}

\usepackage{graphicx}
\usepackage{bbm}
\usepackage{amsfonts,amssymb,amsmath,amsthm}
\usepackage{type1cm,eso-pic,color}
\usepackage{mathrsfs} 
\usepackage{mathpazo}
\usepackage[scaled=.95]{helvet}
\usepackage{courier}
\usepackage{xspace}
\usepackage{etoolbox}
\usepackage[shortlabels]{enumitem}
\usepackage{multirow}
\usepackage{dsfont}
\usepackage{hhline}
\usepackage{graphicx}

\usepackage{xr}
\externaldocument[I-]{../BoundEffRed2}


\input{../mydefs}

\title{Supplementary material for ``An Efficient Forecasting Approach for the Real-Time Reduction of Boundary Effects in Time-Frequency Representations''}

\author{Adrien~Meynard, %~\IEEEmembership{Member,~IEEE,}
        Hau-Tieng~Wu
\thanks{A. Meynard and H.-T. Wu are with the Department
of Mathematics, Duke University, Durham,
NC, 27708 USA e-mail: adrien.meynard@duke.edu}}

\newtheorem{remark}{Remark}

\setcounter{equation}{27}
\setcounter{figure}{7}
\setcounter{table}{3}

\begin{document}
\maketitle


\section{Proof of Lemma~1}
\label{ap:lm.error}

%\subsection{Notations}
Recall the model~\eqref{I-eq:model.noise}. Based on the definition of matrices $\bX$ and $\bY$, we have:
\begin{align}
\dfrac1K\bX\bX^T &= \underbrace{\dfrac1K\bZ\bZ^T+\sigma^2\bI}_{\defeq S^{(0)}} + \bE^{(0)} \\
\dfrac1K\bY\bX^T &= \underbrace{\dfrac1K\bZ'\bZ^T +\sigma^2\bD}_{\defeq S^{(1)}}+ \bE^{(1)} \ ,
\end{align}
where $\bE^{(a)} := \sigma\bE_1^{(a)} + \sigma^2\bE_2^{(a)}$,
\[
\bE_1^{(a)}[m,m'] = \dfrac1K\sum_{k=0}^{K-1} \bz[N_0+m+a+k]\bw[N_0+m'+k] + \bw[N_0+m+a+k]\bz[N_0+m'+k]\ ,
\]
and
\[
\bE_2^{(a)}[m,m'] =  \dfrac1K\sum_{k=0}^{K-1} \bw[N_0+m+a+k]\bw[N_0+m'+k] - \delta_{(m+a)m'}\ ,
\]
with $a\in\{0,1\}$.
We call $\bE^{(0)}$ and $\bE^{(1)}$ {\em error matrices} because:
\begin{align*}
\EE\{\bE^{(0)}\} &= \EE\{\bE_1^{(0)}\} = \EE\{\bE_2^{(0)}\} = \bzero \\
\EE\{\bE^{(1)}\} &= \EE\{\bE_1^{(1)}\} = \EE\{\bE_2^{(1)}\} = \bzero\ .
\end{align*}
Thus, 
\begin{align*}
\bA_0 :=\bS^{(1)}{\bS^{(0)}}^\inv\,,\quad\tilde\bA := (\bS^{(1)}+\bE^{(1)})(\bS^{(0)}+\bE^{(0)})^\inv\,.
\end{align*}
As a result, for $\ell\in \mathbb{N}$, 
\begin{align}
\nonumber
\bh^{(\ell)} &= \balpha^{(\ell)} - \balpha^{(\ell)}_0 \\
\nonumber
&= \be_M^T\left(\tilde\bA^\ell-\bA_0^\ell\right) \\
&= \be_M^T\left(\left((\bS^{(1)}+\bE^{(1)})(\bS^{(0)}+\bE^{(0)})^\inv\right)^\ell- \bA_0^\ell\right)\ .
\label{eq:error.vec}
\end{align}

%\subsection{Study of $\bh^{(\ell)}$}
The randomness of $\bh^{(\ell)}$ completely comes from the error matrices. Besides, notice that the first $M-1$ rows in $\bE^{(1)}$ equal to the last $M-1$ rows of $\bE^{(0)}$. We gather all sources of randomness into an vector $\bg\in\RR^{M(M+1)}$, containing $M$ rows defined as
\[
\bg = \vec\left(
\begin{bmatrix}
\bE^{(0)} \\
\be_M^T\bE^{(1)}
\end{bmatrix}
\right)\,,
\]
where "$\vec$" denotes the vectorization operator, concatenating the columns of a given matrix on top of one another. Then, by definition, we have:
\[
\bg = \sigma\bg_1 + \sigma^2\bg_2\ ,
\]
where:
\begin{align*}
\bg_1 &= \dfrac1K\sum_{k=0}^{K-1}\vec\left( \tilde\bz_k\bw_k^T + \tilde\bw_k\bz_k^T \right) \\
\bg_2 &= \dfrac1K\sum_{k=0}^{K-1}\vec\left(\tilde\bw_k\bw_k^T-\tilde\bI \right) \,,
\end{align*}
where $\tilde\bz_k^T= \begin{pmatrix} \bz_k^T & \bz_{k+1}[M-1] \end{pmatrix}$ and $\tilde\bw_k^T= \begin{pmatrix} \bw_k^T & \bw_{k+1}[M-1] \end{pmatrix}$. Then, $\bg_1$ is a Gaussian random vector because it is a linear combination of Gaussian random vectors. Moreover, using the central limit theorem under weak dependence, we can show that $\bg_2$ also converges towards a Gaussian random vector as $K\to\infty$. Combining these two results leads to
\[
\sqrt{K}\ \bg \xrightarrow[K\to\infty]{\cD} \cN(\bzero,\bGamma_0)\ ,
\]
where $\bGamma_0=\EE\left\{\bg\bg^T\right\}$ is a covariance matrix.

Furthermore, one can write $\bh^{(\ell)}$ as $\bh^{(\ell)}=f^{(\ell)}(\bg)$ where $f^{(\ell)}$ is a deterministic function such that:
\begin{align*}
f^{(\ell)} : \RR^{M(M+1)} &\to \RR^{M} \\
 \bg &\mapsto \bh^{(\ell)}\ . \\
\end{align*}
Then, as $f^{(\ell)}$ is a differentiable function with non-zero differentiation at $0$ {\color{red}[I assume that you have checked all conditions for the delta method. You may consider adding its formula to save the readers' time]}, using the Delta method gives:
\begin{align*}
\sqrt{K}\ \bh^{(\ell)} \xrightarrow[K\to\infty]{\cD} \cN(\bzero,{\bF^{(\ell)}}^T\bGamma_0\bF^{(\ell)})\ ,
\end{align*}
where $\bF^{(\ell)}$ is the Jacobian matrix such that:
\[
\bF^{(\ell)}[m,m'] = \left.\dfrac{\partial f^{(\ell)}_m}{\partial\bg[m']}\right\vert_{\bg=\bzero}\ .
\]

\section{Proof of Theorem~1}
\label{ap:th.error}

\subsection{Expression of the bias $\bmu$.}
Clearly, $\bmu[n]=0$ when $n\in I$. When $n=N-1+\ell$, we have
\begin{align*}
\bmu[n] &=  \EE\{\balpha^{(\ell)}\}\bz_{K} + \sigma\EE\{\balpha^{(\ell)}\bw_{K}\} - \bz[n]\\
& = \balpha_0^{(\ell)}\bz_{K} + \EE\{\bh^{(\ell)}\}\bz_{K} + \sigma\EE\{\bh^{(\ell)}\bw_{K}\} - \bz[N-1+\ell]
\end{align*}
Let us first evaluate the expression of $\balpha_0^{(\ell)}\bz_K$. We have
\begin{align*}
\bS^{(a)}[m,m'] &= \sigma^2\delta_{(m+a)m'}+\sum_{j,j'=1}^J\dfrac{\Omega_j\Omega_{j'}}{K}\sum_{k=0}^{K-1} \cos\left(2\pi \frac{f_j}{\fs}(N_0+m+a+k)+\varphi_j\right)\cos\left(2\pi \frac{f_{j'}}{\fs}(N_0+m'+k)+\varphi_{j'}\right) \\
&= \sigma^2\delta_{(m+a)m'}+\sum_{j=1}^J\dfrac{\Omega_j^2}{2K}\sum_{k=0}^{K-1} \cos\left(2\pi \frac{f_j}{\fs}(m+a-m')\right) + \cos\left(2\pi \frac{f_j}{\fs}(2k+m+a+m'+2N_0)\right)\\
&= \sigma^2\delta_{(m+a)m'}+\sum_{j=1}^J\left( \dfrac{\Omega_j^2}2\cos\left(2\pi \frac{f_j}{\fs}(m+a-m')\right) + \dfrac{\Omega_j^2}{2K}\underbrace{\sum_{k=0}^{K-1}\cos\left(2\pi \frac{f_j}{\fs}(2k+m+a+m'+2N_0)\right)}_{=0\ \mathrm{because}\ \frac{f_j}\fs=\frac{p'_j}K } \right)\\
&= \sigma^2\delta_{(m+a)m'}+\sum_{j=1}^J\dfrac{\Omega_j^2}2\cos\left(2\pi \frac{f_j}{\fs}(m+a-m')\right)\ .
\end{align*}
Thus, $\bS^{(0)}$ is a circulant matrix and is therefore diagonalizable in the Fourier basis:
\[
\bS^{(0)} = \bU\bLambda^{(0)}\bU^*\ ,
\]
where $\bU[m,m']=\frac1{\sqrt{M}}e^{-2\ii\pi mm'/M}$ and $\bLambda^{(0)} = \mathrm{diag}(\lambda_0^{(0)},\dots,\lambda_{M-1}^{(0)})$ with
\begin{align*}
\lambda_m^{(0)} &= \sigma^2 + \sum_{j=1}^J\dfrac{\Omega_j^2}2\sum_{q=0}^{M-1} \cos\left(2\pi\frac{f_j}{\fs} q\right) e^{-2\ii\pi qm/M} \\
&= \sigma^2 + \dfrac{M}{4}\sum_{j=1}^J\Omega_j^2(\delta_{m,p_j} + \delta_{m,M-p_j})\ .
\end{align*}
Therefore,
\begin{align*}
{\bS^{(0)}}^\inv  &= \bU{\bLambda^{(0)}}^\inv\bU^*\,,
\end{align*}
which leads to
\begin{align*}
{\bS^{(0)}}^\inv[m,m']  &= \dfrac1{\sigma^2}\delta_{m,m'}-\sum_{j=1}^J\dfrac{\Omega_j^2}{2\sigma^2(\sigma^2+\Omega_j^2M/4)}\cos\left(2\pi p_j \dfrac{m-m'}{M}\right)\,.
\end{align*}
Consequently, we have
\begin{align}
\nonumber
\tilde\bA_0[m,m']  &= \sum_{q=0}^{M-1} {\bS^{(1)}}[m,q]{\bS^{(0)}}^\inv[q,m'] \\
&= \delta_{m+1,m'} + \sum_{j=1}^J\dfrac{2\Omega_j^2}{\Omega_j^2M+4\sigma^2}\cos\left(2\pi p_j\frac{m'}{M}\right)\delta_{m+1,M}\,.
\label{eq:A0.sine}
\end{align}
Thus,
\begin{align*}
\tilde\balpha_0^{(1)}[m]  &=\sum_{j=1}^J\dfrac{2\Omega_j^2}{\Omega_j^2M+4\sigma^2}\cos\left(2\pi p_j\frac{m}{M}\right) \\
&= \dfrac{2}{M}\sum_{j=1}^J\cos\left(2\pi p_j\frac{m}{M}\right) + o(\sigma)\ .
\end{align*}
Besides, from equation~\eqref{eq:A0.sine}, we have
\begin{align*}
\tilde\bA_0\bz_{K} &= 
\begin{pmatrix}
\bz[N-M+1] \\
\vdots \\
\bz[N-1] \\
\balpha_0^{(1)}\bz_K
\end{pmatrix}\,.
\end{align*}
By induction, we have
\begin{align*}
\tilde\bA_0^{\ell}\bz_{K} &= 
\begin{pmatrix}
\bz[N-M+\ell] \\
\vdots \\
\bz[N-1] \\
\balpha_0^{(1)}\bz_K \\
\vdots \\
\balpha_0^{(\ell)}\bz_K
\end{pmatrix}
\, .
\end{align*}
Then,
\begin{align}
\nonumber
\balpha_0^{(\ell)}\bz_{K} &= \tilde\balpha_0^{(1)}\tilde\bA_0^{\ell-1}\bz_{K} \\
&=\sum_{m=0}^{M-\ell}\balpha_0^{(1)}[m]\bz[N-M+\ell+m-1]+\sum_{m=M-\ell+1}^{M-1}\balpha_0^{(1)}[m]\balpha_0^{(m-M+\ell)}\bz_K\,.
\label{eq:seq}
\end{align}
But,
\begin{align*}
\balpha_0^{(1)}\bz_{K} &=\sum_{m=0}^{M-1}\balpha_0^{(1)}[m]\bz[N-M+m] \\
&=\sum_{j,j'=1}^J\Omega_{j'}\dfrac{2}{M}\underbrace{\sum_{m=0}^{M-1}\cos\left(2\pi p_j\frac{m}{M}\right)\cos\left(2\pi p_{j'}\dfrac{N+m}{M}+\varphi_{j'}\right)}_{=\delta_{j,j'}\frac{M}2\cos\left(2\pi p_j\frac{N}M +\varphi_{j}\right)} + o(\sigma) \\
&= \sum_{j=1}^J\Omega_j\cos\left(2\pi p_j\dfrac{N}{M}+\varphi_j\right) + o(\sigma) \\
&= \bz[N] + o(\sigma)
\end{align*}
By induction from~\eqref{eq:seq}, we have
\begin{equation}
\label{eq:alpha0z.sine}
\balpha_0^{(\ell)}\bz_{K} = \bz[N-1+\ell] + o(\sigma)
\end{equation}
Then,
\begin{equation*}
\bmu[N-1+\ell] = \EE\{\bh^{(\ell)}\}\bz_{K} + \sigma\EE\{\bh^{(\ell)}\bw_{K}\} + o(\sigma)\ .
\end{equation*}
Besides, from Lemma~1, we have the following results:
\begin{align*}
\EE\{\bh^{(\ell)}\} &\underset{K\to\infty}{\longrightarrow} 0\\
\EE\{\bh^{(\ell)}\bw_{K}\} &\underset{K\to\infty}{\longrightarrow} 0
\end{align*}
{\color{red}[if it is possible to get the convergence rate, like $\EE\{\bh^{(\ell)}\}=O(1/K)$, that would be great!]} Consequently,
\begin{equation}
\frac{1}{\sigma}\bmu[N-1+\ell] = o(1)
\label{eq:bias.ap}
\end{equation}
when $K\to\infty$.


\subsection{Expression of the covariance $\bgamma$.}
Take $n>N$, and denote $n=N-1+\ell$.  To derive the covariance, let us segregate the cases. 
 
\paragraph{When $n'\in I$}
From Lemma~1, we have that $h^{(\ell)}$ is asymptotically Gaussian when $K\to \infty$. Then, as a direct consequence of the Isserlis' theorem, odd-order moments are vanishing {\color{red}[there is a gap here. Note that when $K$ is finite, it is approximated by Gaussian but not Gaussian. The discrepancy should be made clear.]}. Then, combining result~\eqref{eq:bias.ap} and equation~\eqref{I-eq:bias0}, we obtain:
\begin{align*}
\bgamma[n,n'] &= \sigma\EE\{\bw[n']\bh^{(\ell)}\}\bz_K + \sigma^2\balpha_0^{(\ell)}\EE\{\bw_K\bw[n']\} + o(\sigma^2) 
\end{align*}
when $K\to \infty$.
We remark that
\[
\balpha_0^{(\ell)}\EE\{\bw_K\bw[n']\}=\balpha_0^{(\ell)}[n'-(N-M)]\1_{(n'\geq N-M)}\ .
\]
Moreover, using Delta method one can show that we have the asymptotic result
\begin{equation}
\EE\{\bw[n']\bh^{(\ell)}\} \underset{K\to\infty}{\longrightarrow} 0\,.
\label{eq:cov.ap}
\end{equation}
As a result, we have
\begin{align*}
\bgamma[n,n'] &= \sigma^2\balpha_0^{(\ell)}[n'-(N-M)]\1_{(n'\geq N-M)}+ o(\sigma^2)
\end{align*}
when $K\to\infty$. {\color{red}[Note that $o(\sigma^2)$ becomes $0$ when $K\to \infty$. So use either ``$\bgamma[n,n']= \sigma^2\balpha_0^{(\ell)}[n'-(N-M)]\1_{(n'\geq N-M)}+ o(\sigma^2)$ when $K\to \infty$'' or ``$\bgamma[n,n'] \to \sigma^2\balpha_0^{(\ell)}[n'-(N-M)]\1_{(n'\geq N-M)}$ when $K\to \infty$''.]}


\paragraph{When $n'\geq N$}
From Lemma~1, we have that $h^{(\ell)}$ is asymptotically Gaussian. Then, as a direct consequence of the Isserlis' theorem, odd-order moments are vanishing. {\color{red}[The same gap here.]} Then, combining equations~\eqref{I-eq:cov01}, \eqref{eq:bias.ap} and~\eqref{eq:cov.ap}, we obtain
\begin{align*}  
\bgamma[n,n'] = &\,\bz_K^T\EE\left\{{\balpha^{(\ell)}}^T\balpha^{(\lambda)}\right\}\bz_K + \sigma\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\}\bz_K+ \sigma\EE\{\balpha^{(\lambda)}\bw_K\balpha^{(\ell)}\}\bz_K \\
& +\sigma^2\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\bw_K\}-\bz[n]\bz[n']+o(\sigma^2) \\
= &\,\bz_K^T\EE\left\{{\bh^{(\ell)}}^T\bh^{(\lambda)}\right\}\bz_K + \balpha_0^{(\ell)}\sigma\EE\{\bw_K\bh^{(\lambda)}\}\bz_K + \sigma\EE\{\bh^{(\ell)}\bw_K\}\bz[n']  \\
&+ \sigma\EE\{\bh^{(\ell)}\bw_K\bh^{(\lambda)}\}\bz_K+ \sigma\EE\{\bh^{(\lambda)}\bw_K\}\bz[n] + \sigma\balpha_0^{(\lambda)}\EE\{\bw_K\bh^{(\ell)}\}\bz_K \\
& + \sigma\EE\{\bh^{(\lambda)}\bw_K\bh^{(\ell)}\}\bz_K+\sigma^2\balpha_0^{(\ell)}\EE\{\bw_K\bh^{(\lambda)}\bw_K\}+\sigma^2\balpha_0^{(\lambda)}\EE\{\bw_K\bh^{(\ell)}\bw_K\} \\
&+\sigma^2\left\langle\balpha^{(\ell)},\balpha^{(\lambda)}\right\rangle +\sigma^2\EE\{\bh^{(\ell)}\bw_K\bh^{(\lambda)}\bw_K\} + o(\sigma^2) \\
= &\,{\color{blue}\dfrac1K\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K} +\sigma^2\left\langle\balpha^{(\ell)},\balpha^{(\lambda)}\right\rangle +\sigma^2\EE\{\bh^{(\ell)}\bw_K\bh^{(\lambda)}\bw_K\} + o(\sigma^2)
\end{align*}
when $K\to\infty$ {\color{red}[in the last equation, make clear which term depends on which equation \eqref{I-eq:cov01}, \eqref{eq:bias.ap} and~\eqref{eq:cov.ap}.] [The previous or current asymptotic expressions with $\dfrac1K\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K$ are both not ideal. Either using your previous notation or following what I prefer here, the behavior of $\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K$ should be made clear. Is it bounded? If it is bounded, then use $=O(1/K)$ to replace $\to \dfrac1K\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K$. If not, how does it depend on $K$ should be made clear.] [Moreover, if you prefer to use your previous notation, you may clarify what it means by $\sim$.]}.
The Isserlis' theorem also gives the following result {\color{red}[the same gap here.]}
\begin{align*}
\EE\{\bh^{(\ell)}\bw_K\bh^{(\lambda)}\bw_K\} &= \sum_{m,m'=0}^{M-1}\EE\{\bh^{(\ell)}[m]\bw_K[m]\bh^{(\lambda)}[m']\bw_K[m']\} \\
&= \sum_{m,m'=0}^{M-1}\EE\{\bh^{(\ell)}[m]\bw_K[m]\}\EE\{\bh^{(\lambda)}[m']\bw_K[m']\} + \EE\{\bh^{(\ell)}[m]\bw_K[m']\}\EE\{\bh^{(\lambda)}[m']\bw_K[m]\} \\
&\hspace{35pt} + \EE\{\bh^{(\ell)}[m]\bh^{(\lambda)}[m']\}\underbrace{\EE\{\bw_K[m]\bw_K[m']\}}_{=\delta_{m,m'}} \\
&\to \sum_{m=0}^{M-1} \EE\{\bh^{(\ell)}[m]\bh^{(\lambda)}[m]\} = \dfrac1K\Tr\left(\bGamma^{(\ell,\lambda)}\right)
\end{align*}
when $K\to \infty$.
We thus conclude that
\begin{align*}  
\bgamma[n,n'] &\to {\color{blue}\dfrac1K\bz_K^T\bGamma^{(\ell,\lambda)}\bz_K} +\sigma^2\left\langle\balpha^{(\ell)},\balpha^{(\lambda)}\right\rangle +\dfrac{\sigma^2}{K}\Tr\left(\bGamma^{(\ell,\lambda)}\right) + o(\sigma^2)
\end{align*}
when $K\to \infty$.


\section{Application to an Electrocardiogram}
We provide here an additional implementation of {\sf BoundEffRed}, applied to an electrocardiogram (ECG) dataset. The dataset is constructed from a $500$-second-long ECG, sampled at $\fs=200$~Hz, cut into $10$ segments of $50$ seconds each. Fig.~\ref{fig:ecg} depicts the right boundary of one of these subsignals, together with the $6$-second extensions estimated by {\sf SigExt} (top panel), or EDMD (middle panel), or GPR (bottom panel). These extensions are superimposed to the ground-truth extension, plotted in red. The sharp and spiky ECG patterns make the AHM model too simplistic to describe this type of signal. Consequently, the forecast produced by {\sf SigExt} is moderately satisfactory. 

\begin{figure}
\includegraphics[width=\textwidth]{ECGforecast.eps}
\caption{Extended ECG (blue) obtained by the {\sf SigExt} forecasting (top), the EDMD forecasting (middle), and the GPR forecasting (bottom), superimposed with the ground truth signal (red).}
\label{fig:ecg}
\end{figure}

Table~\ref{tab:otd.ecg} contains the median performance index $D$ of the boundary-free TF representations, over the $N$ subsignals evaluated, according to the extension method. As a result of the fair quality of the forecasts, the reduction of boundary effects is less significant than for PPG signal. Nevertheles, the results show that {\sf BoundEffRed} has the same efficiency when the {\sf SigExt} extension, the EDMD extension or the GPR extension is chosen. This justifies the choice of {\sf SigExt} for real-time implementation.

\begin{table}
\centering
\caption{ECG: Performance of the Boundary-Free TF Representations According to the Extension Method}
\begin{tabular}{|c||c|c|c|c|}
  \hline
   \multirow{2}{40pt}{\centering Extension method} & \multicolumn{4}{c|}{Median performance index $D$} \\
   \cline{2-5}
      & STFT & SST & ConceFT & RS\\
   \hhline{|=#=|=|=|=|}
   {\sf SigExt} & $0.584$ & $0.630$ & $0.462$ & $0.642$ \\
   \hline
   Symmetric & $1.199$ & $1.354$ & $1.427$ & $0.943$ \\
   \hline
   EDMD & $0.538$ & $0.558$ & $0.496$ & $0.714$ \\
   \hline
   GPR & $0.639$ & $0.588$ & $0.485$ & $0.616$ \\
   \hline
\end{tabular}
\label{tab:otd.ecg}
\end{table}

\end{document}