As explained above, the algorithm for the reduction of boundary effects on TF representations relies on extending the signal by forecasting it before applying the TF analysis. 


We start with the notation. Let $x:\RR\to\RR$ denote a continuous-time signal. In this work, we consider a finite-length discretization of that one. Thus, the sampled signal $\bx$, whose length is denoted by $N$, is such that
\[
\bx[n] = x\left(\frac{n}{\fs}\right)\ ,\quad \forall n\in\{0,\ldots,N-1\}\ , 
\]
where $\fs$ denotes the sampling frequency. 
Let $M$ and $K$ be two positive integers such that $M<K$ and $K+M\leq N$. Then, for all $k\in\{0,\ldots,K-1\}$, we extract from $\bx\in\RR^N$ the subsignal $\bx_k\in\RR^M$ given by:
\begin{equation}
\bx_k = 
\begin{pmatrix}
\bx[N-K-M+k] \\
\vdots \\
\bx[N-K+k-1]
\end{pmatrix}\ .
\label{eq:xk}
\end{equation} 
These subsignals are gathered into the matrix $\bX\in\RR^{M\times K}$ such that:
\begin{equation*}
\bX = 
\begin{pmatrix}
\bx_0 & \cdots & \bx_{K-1}
\end{pmatrix}\ .
\end{equation*}
Notice that these subsignals are overlapping each other. Indeed, $\bx_{k+1}$ is a shifting of $\bx_k$ from one sample. We also consider the matrix $\bY\in\RR^{M\times K}$ given by:
\begin{equation*}
\bY = 
\begin{pmatrix}
\bx_1 & \cdots & \bx_{K}
\end{pmatrix}\ .
\end{equation*}
The boundary effect reduction algorithm is based on manipulating $\bX$ and $\bY$.

%
The pseudo-code of the proposed real-time algorithm to reduce boundary effects on windowing-based TF representations is shown in Algorithm~\ref{alg:boundary}. We coined the algorithm {\sf BoundEffRed}. Below, we detail the algorithm, particularly the signal extension {\sf SigExt} in Algorithm~\ref{alg:extension}.


\begin{algorithm}
\caption{Tackling boundary effects of a TF representation in real-time. $\bF_\bx = \mbox{\sf BoundEffRed}(\bx,M,K,L,\ccF)$}
\label{alg:boundary}
\begin{algorithmic}
\STATE {\bf Inputs}: $\bx_{N}$, $M$, $K$, $N_0$, $L$, $\ccF$
%\STATE {\bf Initialization}: $\bF_\bx^{(N_0)}=\ccF(\bx_{N_0})$
\STATE \vspace{-2mm}
\WHILE {$N$ increases}
\STATE {\bf Real-time input}: $\bx_N$
\STATE \vspace{-2mm}
\STATE {\bf Forecasting step.}
\STATE \quad\textbullet\ Signal extension: $\tilde\bx_{N+L} = \mbox{\sf SigExt}(\bx_N)$. 
\STATE {\bf Representation estimation step.}
\STATE \quad\textbullet\ Extended representation evaluation: $\ccF(\tilde\bx_{N+L})$.
\STATE \quad\textbullet\ Restriction of $\ccF(\tilde\bx_{N+L})$ to the current time interval (see~\eqref{eq:restriction}) to obtain $\bF_\bx^{(N)}=\ccF^{\mathrm{ext}}(\bx_N)$.
\STATE \vspace{-2mm}
\STATE {\bf Real-time output}: Signal representation $\bF_\bx^{(N)}$
%\STATE $N \leftarrow N+1$
\ENDWHILE
\end{algorithmic}
\end{algorithm}


\subsection{Step 1: Extension by Forecasting}



\paragraph{Dynamical Model} 
Establishing a dynamical model consists of determining the relation linking $\bY$ to $\bX$, that is finding a function $f$ so that
\begin{equation}
\bY = f(\bX)\ .
\label{eq:generic.model}
\end{equation}
In a general framework, forecasting means estimating the function $f$ from the observed values taken by the signal, in order to predict its future values. For instance, the dynamic mode decomposition~\cite{Schmid10dynamic,Williams15data} or other more complicated models \cite{Roberts13Gaussian,DeLivera11forecasting,west2006bayesian,vlachas2018data}, allow this by setting additional constraints on the behavior of $f$. We will see, in section~\ref{se:theoretical}, that considering such a complex dynamic model is not necessary for the study of the oscillatory signals of interest to us. That is why we consider here a naive dynamical model, assuming that we have the following relation:
\begin{equation}
\bY = \bA\bX\ ,
\label{eq:dyn.model}
\end{equation}
where $\bA\in\RR^{M\times M}$. In other words, we adopt a classical strategy in the study of dynamical systems, the {\em linearization of a nonlinear system}, when the system is sufficiently regular. Notice that this linearized dynamical model can be written equivalently in function of the subsignals $\bx_k$, as:
\begin{equation}
\bx_{k+1} = \bA\bx_{k},\ ,\forall k\in\{0,\dots,K-1\}\ .
\end{equation}
From the Takens' embedding perspective~\cite{Takens81detecting}, $\bX$ can be viewed as a lag map. Under the manifold assumption of the intrinsic phase space that hosts the dynamics, $f$ describes the dynamics supported on the manifold space, and $\bA$ approximates the dynamics on the manifold. 


\paragraph{Forecasting}
The forecasting method consists in estimating the unknown matrix $\bA$. Indeed, let $\tilde\bA$ denotes the estimate of $\bA$. We then obtain the forecasting of the signal at time $\frac{N-1+\ell}{\fs}$ by:
\begin{equation}
\label{eq:prediction}
\tilde \bx [N-1+\ell] = \balpha^{(\ell)}\bx_{K}\ ,
\end{equation}  
where $\balpha^{(\ell)}$ denotes the last row of $\tilde\bA^\ell$; that is to say,
\begin{equation}
\balpha^{(\ell)}=\be_M^T\tilde\bA^\ell\ ,
\label{eq:alpha.l}
\end{equation}
where $\be_M$ is the unit vector of length $M$ given by $\be_M = \begin{pmatrix} 0 & \cdots & 0 & 1\end{pmatrix}^T$.

\paragraph{Model Estimation} To estimate the matrix $\bA$, we consider the simple but numerically efficient least square estimator. That is, we solve the following problem:
\begin{equation}
\label{eq:ls.pb}
\tilde\bA = \arg\min_{\balpha} \cL(\bA)\ ,
\end{equation}
where the loss function $\cL$ is given by:
\[
\cL(\bA) = \|\bY-\bA\bX\|^2 = \sum_{k=0}^{K-1} \|\bx_{k+1}-\bA\bx_k\|^2.
\]
Therefore, solving the problem~\eqref{eq:ls.pb}, \ie~$\nabla \cL(\tilde\bA)=\bzero$, gives the following estimate $\tilde\bA$ of the dynamical model matrix $\bA$:
\begin{align}
\tilde\bA &= \bY\bX^T(\bX\bX^T)^{-1}\ .
\label{eq:lse}
\end{align}

\begin{remark}
This expression clearly shows that the matrix $\tilde\bA$ takes the following form:
\[
\tilde\bA =
\begin{pmatrix}
0       & 1       & 0      & \cdots & 0      \\
\vdots  & \ddots  & \ddots & \ddots & \vdots  \\
\vdots  &         & \ddots & \ddots & 0  \\
0       & \cdots  & \cdots & 0      & 1  \\
\alpha_1& \cdots  & \cdots & \cdots & \alpha_M  \\
\end{pmatrix}.
\]
Then, except for the row vector $\balpha = \left(\alpha_1 \cdots\alpha_M\right)$, the matrix $\bA$ is fully determined by the dynamical model.
\end{remark}

\paragraph{Signal Extension} Since we are building a real-time algorithm, we consider that only the right ``side'' of the signal has to be extended, the left ``side'' being fixed since it only concerns the past values of the signal. We therefore construct the extended signal $\tilde\bx\in\RR^{N+L}$ concatenating the observed signal $\bx$, and the forward forecasting $\tilde\bx_{\mathrm{fw}}\in\RR^L$. We summarize the extension step in Algorithm~\ref{alg:extension}. %We mention that we could, if necessary, handle the backward estimation using the same strategy as described above, but applying it to the reverse signal $\bx^{\rm r} = \begin{pmatrix} \bx[N-1] & \cdots & \bx[0] \end{pmatrix}^T$.

\begin{algorithm}
\caption{Signal extension. $\tilde\bx = \mbox{\sf SigExt}(\bx,M,K,L)$}
\label{alg:extension}
\begin{algorithmic}
\STATE {\bf Inputs}: $\bx_N$, $M$, $K$, $L$
\STATE \vspace{-2mm}
\STATE {\bf Forward forecasting.}
\STATE \quad\textbullet\ Estimation of the matrix $\tilde\bA$ via equation~\eqref{eq:lse}.
\STATE \quad\textbullet\ Forecasting $\tilde\bx_{\mathrm{fw}}\in\RR^L$ obtained applying equation~\eqref{eq:prediction} with $\ell\in\{1,\ldots,L\}$.
%\STATE 
%\STATE {\bf Backward forecasting.}
%\STATE \quad\textbullet\ Reverse signal $\bx$ to $\bx^{\rm r}$. 
%\STATE \quad\textbullet\ LS estimation of the backward matrix $\tilde\bA_{\rm bw}$ via equation~\eqref{eq:lse} applied to $\bx^{\rm r}$.
%\STATE \quad\textbullet\ Reversed backward forecasting $\tilde\bx_{\rm bw}^{\rm r}$ obtained applying equation~\eqref{eq:prediction} to $\bx^r$ with $\ell\in\{1,\ldots,L\}$.
%\STATE \quad\textbullet\ Reverse $\tilde\bx_{\rm bw}^{\rm r}$ to obtain the estimate $\tilde\bx_{\rm bw}$.
\STATE \vspace{-2mm}
\STATE {\bf Output}: Extended signal $\tilde\bx_{N+L} = \begin{pmatrix} \bx_N & \tilde\bx_{\mathrm{fw}}\end{pmatrix}^T$. %\tilde\bx_{\rm bw}  &
\end{algorithmic}
\end{algorithm}

\subsection{Step 2: Extended Time-Frequency Representation}
\label{sse:extension.TFR}
 
Let $\ccF:\RR^{N}\to\CC^{F\times N}$ generically denotes the TF representation of interest to us, which could be, for instance, STFT, CWT, SST, or RS. Here, $F$ typically denotes the size of the discretization along the frequency axis. Due to the boundary effects, the representation $\ccF(\bx_N)$ shows undesired patterns near its edges. To alleviate the boundary effects, we apply the representation to the estimated extended signal $\tilde\bx$. This strategy moves the boundary effects out of the time interval $I=[0, \frac{N-1}{\fs}]$. Finally, the boundary-effects insensitive representation $\ccF^{\mathrm{ext}}:\RR^{N}\to\CC^{F\times N}$ of $\bx_N$ is given for all $\nu\in\{0,\cdots,F-1\}$, $n\in\{0,\cdots,N-1\}$ by:
\begin{equation}
\ccF^{\mathrm{ext}}(\bx_N)[\nu,n] = \ccF(\tilde \bx_{N+L})[\nu,n]\ .
\label{eq:restriction}
\end{equation}
This amounts to restricting the representation $\ccF(\tilde \bx_{N+L})$ to the current measurement interval of $\bx_N$. For the sake of simplicity, we denote the {\em restriction operator} by $\cR$, where $\cR:\CC^{F\times (N+L)}\to\CC^{F\times N}$. Consequently, we have:
\begin{equation}
\ccF^{\mathrm{ext}}(\bx_N) = \cR\left( \ccF(\tilde \bx_{N+L}) \right) \ .
\label{eq:bound.free.TFR}
\end{equation}
We call $\ccF^{\mathrm{ext}}$ the {\em boundary-free TF representation}.

$\bF_\bx^{(N)}\in\CC^{F\times N}$ is the estimation of the boundary-free TF representation at iteration $N$ in Algorithm~\ref{alg:boundary}. For numerical purposes, and to make the real-time implementation achievable, we do not perform a full re-estimation of $\ccF^{\mathrm{ext}}$ at iteration $N+1$. Instead, the additional knowledge provided by $\bx[N+1]$ only influences the values of the last $L_{\mathrm{win}}$ columns of $\ccF^{\mathrm{ext}}(\bx_{N+1})$, where $L_{\mathrm{win}}$ denotes the window half-length used by the TF representation. Thus, $\bF_\bx^{(N+1)}$ is obtained by the concatenation of the first $N-L_{\mathrm{win}}+ 1$ columns of $\bF_\bx^{(N)}$ with the last $L_{\mathrm{win}}$ columns of $\ccF^{\mathrm{ext}}(\bx_{N+1})$.