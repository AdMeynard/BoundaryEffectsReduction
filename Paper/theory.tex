\subsection{Signal Model}
\label{sse:model.sine}
We model the meaningful part of the observed signal as a deterministic multicomponent harmonic signal; that is, a sum of sine waves:
\begin{equation}
\bz[n]=\sum_{j=1}^J\Omega_j\cos\left(2\pi f_j \frac{n}{\fs} + \varphi_j\right)\ ,
\label{eq:sum.sine}
\end{equation}
where $J$ denotes the number of components, $\Omega_j>0$ the amplitude of the $j$-th component, $f_j$ its frequency, and $\varphi_j\in[0,2\pi)$ its initial phase.
%
For the sake of simplicity, we make an additional assumption on the frequencies of each component. We assume that for all $j\in\{1,\dots,J\}$:
\begin{equation}
\exists\, p_j,p_j'\in\NN^*: \quad f_j = \dfrac{p_j}{M}\fs = \dfrac{p'_j}{K}\fs\ .
\end{equation}


In addition, the observed signal is assumed to be corrupted by an additive Gaussian white noise. Therefore, the measured discrete signal $\bx$ is written as:
\begin{equation}
\bx = \bz + \sigma\bw\ ,
\label{eq:model.noise}
\end{equation}
where $\bz$ follows model~\eqref{eq:sum.sine}, $\bw$ is a Gaussian white noise, whose variance is normalized to one, and $\sigma>0$. Clearly, $\sigma^2$ denotes the variance of the additive noise $\sigma\bw$.


\subsection{Forecasting Error}
On the forecasting interval, we decompose the estimated signal $\tilde\bx$ as follows:
\begin{equation}
\tilde \bx[n] = \bz[n] + \bepsilon[n]\ ,
\label{eq:forecasting.error}
\end{equation}
where $\bepsilon$ is the forecasting error. When $n\in I=\{0,\dots,N-1\}$, this error contains only the measurement noise, that is $\bepsilon[n]=\sigma\bw[n]$. Outside the interval $I$, the importance of the forecasting error $\bepsilon$ is also affected by the loss of information resulting from the linearization of the dynamical model we consider in~\eqref{eq:dyn.model}. To evaluate the actual behavior of the forward forecasting error $\bepsilon[n]$ when $n\geq N$, we determine its first two moments.
\begin{enumerate}
\item 
The mean, or estimation bias, is such that: 
\begin{align}
\bmu[n]&\defeq\EE\{\bepsilon[n]\} 
=\EE\{\tilde\bx[n]\} - \bz[n]\ .\label{eq:defn:bias0}
\end{align}
Given the forecasting strategy, we have $\bmu[n]=0$ when $n\in I$ and
\begin{equation}
\bmu[n]= \EE\{\balpha^{(n-N+1)}\}\bz_{K} + \sigma\EE\{\balpha^{(n-N+1)}\bw_{K}\} - \bz[n]
\label{eq:bias0}
\end{equation}
when $n\geq N$.
\item 
The covariance is given by:
\begin{align*}
\bgamma[n,n'] &\defeq \EE\{\left(\bepsilon[n]-\bmu[n]\right)\left(\bepsilon[n']-\bmu[n']\right)\}\\
&= \EE\{\tilde\bx[n]\tilde\bx[n']\}-\bz[n]\bz[n'] -\bmu[n]\bz[n'] \\
&\hspace{15pt} - \bmu[n']\bz[n] -\bmu[n]\bmu[n']\ .
\end{align*}
Thus by definition of the noise, we have $\bgamma[n,n'] =\sigma^2\delta_{n,n'}$ when $(n,n')\in I^2$. When $n\geq N$, let us denote $\ell=n-N+1$. Then, we have two cases.
\begin{enumerate}[label=(\roman*)]
\item If $n'\in I$:
\begin{equation}
\bgamma[n,n'] = \sigma\EE\{\bw[n']\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\bw[n']\balpha^{(\ell)}\bw_K\}\ .
\label{eq:cov00}
\end{equation}
\item If $n'=N-1+\lambda\geq N$:
\begin{align}
\nonumber
\hspace{-6pt}\bgamma[n,n'] &= \bz_K^T\EE\left\{{\balpha^{(\ell)}}^T\balpha^{(\lambda)}\right\}\bz_K + \sigma\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\}\bz_K \\
\nonumber
&\hspace{-3pt} + \sigma\EE\{\balpha^{(\lambda)}\bw_K\balpha^{(\ell)}\}\bz_K + \sigma^2\EE\{\balpha^{(\ell)}\bw_K\balpha^{(\lambda)}\bw_K\}  \\
&\hspace{-3pt} -\!\bz[n]\bz[n']-\!\bz[n]\bmu[n']-\!\bz[n]\bmu[n'] -\! \bmu[n]\bmu[n']\,.
\label{eq:cov01}
\end{align}
\end{enumerate}
Besides, we recall that $\bgamma[n,n']=\bgamma[n',n]$.
\end{enumerate}

In Theorem~\ref{th:error}, we specify the asymptotic behavior of the forecasting bias and covariance when the dataset size $K$ is great.
\begin{theorem}
\label{th:error}
Let $\bx\in\RR^N$ be a discrete-time random signal following model~\eqref{eq:model.noise}. Let $\tilde\bx$ denotes its forecasting, obtained using the extension Algorithm~\ref{alg:extension}. Let $n\geq N$ be a sample index. Then, the first-order moment of the forecasting error $\epsilon[n]$ in~\eqref{eq:forecasting.error} defined in \eqref{eq:defn:bias0} satisfies
\begin{equation}
\left|\bmu[n]\right| \leq a^{(n)}_0\sigma^2 + \dfrac1K \left( \dfrac{a^{(n)}_1}{\sigma^2} + a^{(n)}_2 \right) + o\left( \dfrac1K \right)
\label{eq:mean.error}
\end{equation}
as $K\to\infty$, where $a^{(n)}_0$, $a^{(n)}_1$, and $a^{(n)}_2$ are positive quantities, independent of $K$ and $\sigma$.
Its second-order moment $\bgamma[n,n']$ satisfies the following approximation equations:
\begin{enumerate}[label=(\roman*)]
\item if $n'\in I=\{0,\ldots,N-1\}$:
\begin{equation}
\left|\bgamma[n,n']\right|\leq b^{(n,n')}_0\sigma^2 + \!\dfrac1K\! \left( b^{(n,n')}_1\! + b^{(n,n')}_2\sigma^2 \! \right) + o\left( \dfrac1K \right)
\label{eq:cov.error.1}
\end{equation}
as $K\to\infty$, where $b^{(n,n')}_0$, $b^{(n,n')}_1$, and $b^{(n,n')}_2$ are positive quantities, independent of $K$ and $\sigma$;
\item if $n'\geq N$:
\begin{align}
\nonumber
\left|\bgamma[n,n']\right| &\leq c^{(n,n')}_0\sigma^2 \\
&\hspace{10pt} + \!\dfrac1K\! \left(\! \dfrac{c^{(n,n')}_1}{\sigma^2} + c^{(n,n')}_2 + c^{(n,n')}_3\sigma^2 \right) + o\!\left( \!\dfrac1K \!\right)
\label{eq:cov.error.2}
\end{align}
as $K\to\infty$, where $c^{(n,n')}_0$, $c^{(n,n')}_1$, $c^{(n,n')}_2$ and $c^{(n,n')}_3$ are positive quantities, independent of $K$ and $\sigma$.
\end{enumerate}
\end{theorem}

\begin{proof}
See the Supplementary Materials. The proof is mainly based on Taylor expansions of the forecasting error. Nonoptimal values of $a^{(n)}_0$, $a^{(n)}_1$,\dots, $b^{(n,n')}_0$,\dots, $c^{(n,n')}_3$ are also provided in the proof. 
\end{proof}
%The Isserlis' theorem~\cite{Isserlis16formula}, which provides a formula for the computation of higher-order moments of Gaussian random variables.

The forecasting bias and covariance asymptotically depend linearly on the ratio $\frac{1}{K}$. This shows the need to use a sufficiently large dataset to obtain an accurate forecast. Ideally when $K\to\infty$, the forecasting error would behave like the measurement noise $\sigma\bw$, \ie~a zero-mean white noise whose variance is of the order of $\sigma^2$. However, Theorem~\ref{th:error} shows that the estimator may remain asymptotically biased (the first term in the bound of equation~\eqref{eq:mean.error} being independent of $K$). Concerning the covariance of the forecasting error, the expected asymptotic behavior is verified. Indeed, when $K \to\infty$, the variance of the forecasting estimator increases at most linearly with the noise variance $\sigma^2$, since is bounded by $b_0^{(n,n')}\sigma^2$.

Assume now that $K$ is sufficiently great and fixed. The noise influences the quality of the forecasting estimator in two ways. On the one hand, when the noise variance $\sigma^2$ is great, the bias and the variance of the estimator potentially increase linearly with $\sigma^2$. This reflects the classical behavior of an estimator of a signal degraded by additive noise. On the other hand, due to terms in $1/\sigma^2$, when the noise variance $\sigma^2$ is small, the bias and the variance of the forecasting estimator are no longer controlled via equations~\eqref{eq:mean.error} and~\eqref{eq:cov.error.2}. This illustrates the necessary presence of noise to obtain an accurate prediction. Indeed, if $\sigma$ is too small, the matrix $\bX\bX^T$ is ill-conditioned and its inversion in~\eqref{eq:lse} is sensitive. The forecasting is then of poor quality. Noise therefore appears as a regularization term to ensure the invertibility of $\bX\bX^T$. This seemingly counterintuitive fact indicates the challenge we encounter to do forecasting with the dynamic model~\eqref{eq:generic.model}, the degeneracy. Note that from the lag map perspective, $\bX$ and $\bY$ consist of points located on a low dimensional manifold. Thus, locally the ranks of $\bX$ and $\bY$ are low, which leads to the degeneracy. However, when noise exists, it naturally fills in deficient ranks, and stabilizes the algorithm. 


The dependencies of the forecasting quality on the subsignals lengths $M$ and the forecasting index $\ell=n-N+1$ are hidden in the expression of the parameters $a^{(n)}_0$, $a^{(n)}_1$,\dots, $b^{(n,n')}_0$,\dots, $c^{(n,n')}_3$. The numerical results, discussed in section~\ref{ssse:res.sine}, allow us to better evaluate these dependencies.

\subsection{Adaptive Harmonic Model}
\label{RemarkAHM}

One can extend the previous result to the case where the instantaneous frequencies and amplitudes of the components of the deterministic part of the observed signal are {\em slowly varying}. It is an \textit{AM-FM model} that, in its is continuous-time version, takes the following form
\begin{equation}
z(t) = \sum_{j=1}^J a_j(t)\cos(2\pi\phi_j(t))\,,
\end{equation}
where $a_j(t)$ and $\phi'_j(t)$ describe how large and fast the signal oscillates at time $t$. 
Clearly, \eqref{eq:sum.sine} is a special case satisfying the AM-FM model when the $a_j$ and $\phi'_j$ are both constants. 
%
In many practical signals, the amplitude and frequency do not change fast. It is thus reasonable to further restrict the regularity and variations of the instantaneous amplitudes and frequencies of these AM-FM functions. When the speeds of variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$ are small, the signal can be ``locally'' well approximated by a harmonic function in \eqref{eq:sum.sine}; that is, 
%
locally at time $t_0$, $z(t)$ can be well approximated by $z_0(t) := \sum_{j=1}^J a_j(t_0)\cos(2\pi(\phi_j(t_0)-t_0\phi'_j(t_0)+\phi'_j(t_0)t))$ by the Taylor expansion. An AM-FM function satisfying the slow variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$ is referred to the adaptive harmonic model (AHM) (see~\cite{Chen14nonparametric,Daubechies16conceft} for mathematical details).
It is thus clear that the forecasting error caused by the {\sf SigExt} algorithm additionally depends on the speed of variation of the instantaneous amplitudes $a_j$ and frequencies $\phi'_j$. 
%
For the forecasting purpose, it is thus clear that when the speed of variation of $a_j$ and $\phi'_j$ are slow $K$ can be chosen large while the signal can be well approximated by~\eqref{eq:sum.sine}. Hence, Theorem~\ref{th:error} can still be applied to approximate the error. However, how to determine the optimal $K$ based on the signal depends on the application and is out of the scope of this paper. It will be explored in future work for specific application problems.
%\end{remark}

The models of sections~\ref{sse:model.sine} and~\ref{RemarkAHM} consider the meaningful part of the signal to be a deterministic component. These models are purposely adapted to signals showing local line spectra. The physiological signals we are interested in, such as respiratory or cardiac signals, have this characteristic (see section~\ref{sse:physio.sig}). Signals with wider local spectra, such as electroencephalography signals, do not fall into this category, and are more faithfully modeled as random signals. Thus, the theoretical justifications proposed above are no longer applicable to guarantee the forecasting quality of {\sf SigExt}.

\subsection{Performance of the Boundary Effects Reduction}
\label{sse:perf.BoundEffRed}
While we do not provide a generic proof of the boundary effect reduction on {\em any} TF analysis tools, we discuss a particular case of SST. Since SST is designed to analyze signals satisfying the AHM, as is discussed in section~\ref{RemarkAHM}, Theorem~\ref{th:error} ensures that the forecasting error $\bepsilon$ defined in~\eqref{eq:forecasting.error} is controlled and bounded in terms of mean and covariance. Recall Theorem 3 in~\cite{Chen14nonparametric}, which states that when the additive noise is stationary and may be modulated by a smooth function with a small fluctuation, the SST of the observed signal remains close to the SST of the clean signal, throughout the TF plane. 
%
We refer the reader to~\cite{Chen14nonparametric} for a precise quantification of the error made in the TF plane, which depends notably on the covariance of the additive noise and on the speed of variation of the amplitudes and instantaneous frequencies composing the signal. 
%
In our case, while the noise of the historical data is stationary, the forecasting error depends on the historical noise, and hence the overall ``noise'' is nonstationary. However, the dependence only appears in the extended part of the signal. We claim that the proof of Theorem 3 in~\cite{Chen14nonparametric} can be generalized to explain the robustness of SST to the noise, and a systematic proof will be reported in our future theoretical work. We verify experimentally that Algorithm~\ref{alg:boundary} is efficient for a large number of representations in the following section~\ref{se:results}. 
%
Therefore, in our case, this means that the boundary effect is strongly reduced since the impact of the forecasting error does SST is limited. An immediate application is that the instantaneous frequencies can now be well estimated continuously up to the edges of the TF plane, and real-time acquisition of the instantaneous frequency and amplitude information is possible.